{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info=tfds.load(name='mnist', as_supervised=True, with_info=True) # As supervised loads the data in a two\n",
    "# tuple structure(inputs and targets). With info provides a tuple containing info about version, features and \n",
    "# the number of samples in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test'] # We define the train and test datasets. \n",
    "# The MNIST dataset does not come with validation samples so we have to do a split. \n",
    "\n",
    "# We take 10 percent of the training dataset as our validation samples\n",
    "mnist_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "\n",
    "# The computation above could result in a number of samples with decimal places which isn't ideal, so we cast the data\n",
    "mnist_validation_samples = tf.cast(mnist_validation_samples, tf.int64)\n",
    "\n",
    "# We do the same for the test data as we would need this later during batching\n",
    "mnist_test_samples = mnist_info.splits['test'].num_examples\n",
    "mnist_test_samples = tf.cast(mnist_test_samples, tf.int64)\n",
    "\n",
    "# To ensure our result is numerically stable, we scale the data set to have inputs between 0 and 1\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32) # To ensure the values are in float as we will be scaling\n",
    "    image /= 255.0 # Dividing by 255 will keep all values between 0 and 1\n",
    "    return image, label\n",
    "\n",
    "scaled_train_validation = mnist_train.map(scale) # This transforms the dataset using the scale function created above\n",
    "scaled_test_data = mnist_test.map(scale) # Same process for the test dataset as we want the test data to have the \n",
    "# same magnitude as that of the training and validation data\n",
    "\n",
    "# Now we shuffle the dataset\n",
    "\n",
    "BUFFER_SIZE = 10000 # We create a buffer size in order to optimize the computer's memory. This buffer will be used to\n",
    "# shuffle 10000 samples per time\n",
    "\n",
    "shuffled_scaled_train_validation = scaled_train_validation.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# We can now split the dataset into training and validation dataset\n",
    "\n",
    "validation_data = shuffled_scaled_train_validation.take(mnist_validation_samples) # Takes 10 percent of the shuffled\n",
    "# data as the validation data\n",
    "training_data = shuffled_scaled_train_validation.skip(mnist_validation_samples) # Skip as many samples as we have in\n",
    "# our validation data\n",
    "\n",
    "# We apply a mini-batch gradient descent with a batch size of 100\n",
    "# The validation data doesn't need to be batched as we don't need to update the weights and biases(backward propagation)\n",
    "# The inputs from the validation data are only pushed through the net(forward propagation), hence we do create mini\n",
    "# batches for our validation data\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "training_data = training_data.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Since our model expects our validation and test data in batch form too, we also batch the data but all the data goes in \n",
    "# a single batch\n",
    "\n",
    "validation_data = validation_data.batch(mnist_validation_samples)\n",
    "test_data = scaled_test_data.batch(mnist_test_samples) # Test data also goes in a single batch\n",
    "\n",
    "# We have a two tuple structure with inputs and targets. Recall enabling as_supervised. We then create an iterable\n",
    "# and load the next batch(there is only one batch)\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First,we define the hyperparameters\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 500 # Assuming all hidden layers are of the same width\n",
    "\n",
    "# Since our input is a tensor of rank 3 (28x28x1), we flatten the input into a vector to feed into the network\n",
    "# We then build a net with 2 hidden layers with the activation function as ReLu and the output unit using the \n",
    "# softmax activation function in order to generate a probabilistic output.\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select optimizers and loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Adaptive moment estimation as our optimizer. The loss function here is selected in order to pass in \n",
    "# integer inputs and transform into one-hot encoding. In addition, we add the accuracy as part of the metrics to \n",
    "# track during the training of our model\n",
    "#custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.02)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 9s - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.0092 - val_accuracy: 0.9970\n",
      "Epoch 2/10\n",
      "540/540 - 9s - loss: 0.0134 - accuracy: 0.9955 - val_loss: 0.0366 - val_accuracy: 0.9883\n",
      "Epoch 3/10\n",
      "540/540 - 9s - loss: 0.0127 - accuracy: 0.9962 - val_loss: 0.0144 - val_accuracy: 0.9950\n",
      "Epoch 4/10\n",
      "540/540 - 9s - loss: 0.0117 - accuracy: 0.9957 - val_loss: 0.0165 - val_accuracy: 0.9938\n",
      "Epoch 5/10\n",
      "540/540 - 9s - loss: 0.0116 - accuracy: 0.9959 - val_loss: 0.0122 - val_accuracy: 0.9960\n",
      "Epoch 6/10\n",
      "540/540 - 9s - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.0143 - val_accuracy: 0.9955\n",
      "Epoch 7/10\n",
      "540/540 - 9s - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.0162 - val_accuracy: 0.9950\n",
      "Epoch 8/10\n",
      "540/540 - 9s - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.0117 - val_accuracy: 0.9965\n",
      "Epoch 9/10\n",
      "540/540 - 10s - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.0055 - val_accuracy: 0.9983\n",
      "Epoch 10/10\n",
      "540/540 - 11s - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0098 - val_accuracy: 0.9967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13ca3bfd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We define the number of epochs\n",
    "NUM_EPOCHS=10\n",
    "\n",
    "model.fit(training_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets),\n",
    "          validation_steps=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 1s 1s/step - loss: 0.1080 - accuracy: 0.9805"
     ]
    }
   ],
   "source": [
    "# We push the test data through the network(forward propagation) in order to obtain the accuracy of the model\n",
    "# The test accuracy tells us the accuracy to expect when we deploy this model in the real world\n",
    "test_loss, test_accuracy = model.evaluate(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.11, Accuracy : 98.05%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss : {0:.2f}, Accuracy : {1:.2f}%\".format(test_loss, test_accuracy*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
